{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing Meteorological Predictor Fields \n",
    "Version 18 January 2024, Selina Kiefer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input: netcdf- or grib-file\n",
    "continuous timeseries of meteorological predictors in netcdf- or grib-format (i.e. ERA-5, various predictor fields (u10, z100, z250, z500, z850, t850, H850, u300, msl), 4 times (00, 06, 12, 18 UTC), 1950 - 2020, Oct-Apr, 60°W-60°E and 20-80°N, e.g. from https://cds.climate.copernicus.eu/#!/search?text=ERA5&type=dataset)\n",
    "### Output: csv-file\n",
    "continuous timeseries of meteorological predictors in csv-format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used software: Climate Data Operators and Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Climate Data Operators (CDO) \n",
    "\n",
    "Tailored open-source software to perform the most-common meteorological operations efficiently (and much faster than Python). \n",
    "\n",
    "Up to date information about CDO: https://code.mpimet.mpg.de/projects/cdo\n",
    "\n",
    "Reference: Schulzweida, U. (2019): \"CDO User Guide\". Available at: https://doi.org/10.5281/ZENODO.3539275."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short introduction to CDO\n",
    "\n",
    "The overall structure for most operations is:\n",
    "\n",
    "cdo -operator_last_executed,optional_specifications -operator_first_executed,optional_specifcations ifile ofile\n",
    "\n",
    "e.g. cdo -daymean -selyear,1950,1951 input_file_name output_file_name\n",
    "\n",
    "The input file (ifile) and the output file (ofile) of one operation have to have different names. So it is best to name all files, which are not intended for further use, similarly, e.g. temp_1, temp_2, etc. and to delete them afterwards directly.\n",
    "\n",
    "CDO does not ask when overwriting an existing file. So make sure that everything is named uniquely and correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with CDO\n",
    "\n",
    "Since it is much faster than Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short overview of the data file's content.\n",
    "!cdo sinfov  ./era5_msl_180W_180E_0N_90N_1950_1978.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed depiction of the data file's content. Use grib_dump for files in grib-format, \n",
    "# nc_dump for files in netcdf-format. It might be wise to use a separate terminal for this\n",
    "# command since it prints all available information about the data file.\n",
    "#! grib_dump ./era5_msl_60W_60E_20N_80N_1950_2020.nc\n",
    "#! nc_dump ./era5_msl_60W_60E_20N_80N_1950_2020.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spatial Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of a gridbox (sellonlatbox,°W,°E,°S,°N). Western longitudes have to be given as \n",
    "# 360°-°W). In case there is only 1 latitude or longitude to average over, select the desired\n",
    "# longitude/latitude and on the second position the desired longitude/latitude+1. Otherwise \n",
    "# CDO may perform not well. \n",
    "! cdo sellonlatbox,300,60,20,80 ./era5_msl_180W_180E_0N_90N_1950_1978.nc temp_11\n",
    "! cdo sellonlatbox,300,60,20,80 ./era5_msl_180W_180E_0N_90N_1979_2020.nc temp_12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of the daily mean (daymean). Set the time to 00 UTC (settime,00:00:00) to avoid \n",
    "# any inconveniences when reading in the data later with python.\n",
    "! cdo -settime,00:00:00 -daymean temp_11 temp_21\n",
    "! cdo -settime,00:00:00 -daymean temp_12 temp_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of certain times, e.g. only the wintermonths (selmon).\n",
    "! cdo -selmon,1,2,3,4,10,11,12 temp_21 temp_31\n",
    "! cdo selmon,1,2,3,4,10,11,12 temp_22 temp_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of only the relevant data according to the lead time at the beginning of each\n",
    "# winter. Number of days to delete = (Days_of_Month - lead_time).\n",
    "! cdo delete,day=1,2,3,month=10 temp_31 temp_41\n",
    "! cdo delete,day=1,2,3,month=10 temp_32 temp_42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of only the relevant data according to the lead time at the end of each winter.\n",
    "# Number of days to delete = lead_time.\n",
    "! cdo delete,day=3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,month=4 temp_41 temp_51\n",
    "! cdo delete,day=3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,month=4 temp_42 temp_52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of only the relevant data according to the lead time at the end of the data in case\n",
    "# it does end with 31 Dec instead of 30 Apr. Number of days to\n",
    "# delete = (Days_of_Month - lead_time).\n",
    "! cdo delete,day=4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,month=12,year=2020 temp_52 temp_62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal merging of two timeseries. The option \"-b F64\" makes sure that the two dataseries \n",
    "# can be combined without errors. \n",
    "! cdo -b F64 mergetime temp_51 temp_62 temp_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the time is sorted correctaly (sorttimestamp) and the file is named correctly.\n",
    "! cdo sorttimestamp temp_7 ./era5_msl_60W_60E_20N_80N_1950_2020_only_Oct_Apr_lead_time_28d.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert from grib-format to netcdf-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the grib-file to a netcdf-file if necessary. The python-scripts are designed to use\n",
    "# netcdf-files.\n",
    "#! cdo -f nc copy ofile.grib ofile.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unnecessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary files which have been created by CDO since the names of the input files \n",
    "# and output files have to be unique.\n",
    "! rm temp*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue with Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a nice overview of the data, pandas dataframes are used. These are then converted directly into csv-format for storage which ensures a safe and easy data transfer between various jupyter notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the paths' and files' names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the needed path and file names.\n",
    "PATH_defined_functions = './Defined_Functions/'\n",
    "\n",
    "PATH_data = './Data_in_Netcdf_Format/'\n",
    "ifile_data = 'era5_msl_60W_60E_20N_80N_1950_2020_only_Oct_Apr_lead_time_28d.nc'\n",
    "\n",
    "PATH_output_file = './Data_in_csv_Format/'\n",
    "file_name_output_file = 'era5_msl_60W_60E_20N_80N_1950_2020_only_Oct_Apr_lead_time_28d.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the necessary packages and functions\n",
    "Nothing needs to be changed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary python packages.\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary functions.\n",
    "import sys\n",
    "sys.path.insert(1,PATH_defined_functions)\n",
    "from read_in_netcdf_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the data and check the file's content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data and show its header.\n",
    "df_data = read_in_netcdf_data(PATH_data, ifile_data)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the end of the dataframe.\n",
    "df_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply a 7-day running mean for temporal aggregation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a 7-day rolling mean for temporal aggregation.\n",
    "df_data_grouped = df_data\n",
    "df_data_grouped = df_data.groupby(['latitude', 'longitude'], as_index=False)\n",
    "df_data_grouped = pd.DataFrame(df_data_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every latitude-longitude pair, apply the 7-day running mean.\n",
    "list_rolling_means = []\n",
    "df_data_grouped = df_data_grouped[1]\n",
    "for i in range(len(df_data_grouped)):\n",
    "    list_rolling_means.append(df_data_grouped.iloc[i]['msl'].rolling(window=7, center=True).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat the list with the rolling means (does the same like \"extend\").\n",
    "rolling_means = pd.concat(list_rolling_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list to a pandas dataframe and sort its index.\n",
    "df_rolling_means = pd.DataFrame(rolling_means)\n",
    "df_rolling_means = df_rolling_means.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the column with the original variable and replace it with the 7-day rolling mean of the variable.\n",
    "df_data = df_data.drop(['msl'], axis=1)\n",
    "df_data['msl'] = df_rolling_means['msl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove any columns containing NaN-Values since the used ML-models cannot handle NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any columns containing NaN-values.\n",
    "df_data = df_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a minimal, useful representation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the month of the winter to include seasonality. Needs to be done only for one predictor.\n",
    "#df_data = df_data.set_index('time')\n",
    "#df_data['month'] = df_data.index.month\n",
    "#df_data = df_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the variable's comlumn in case its naming is ambiguous.\n",
    "df_data = df_data.rename(columns={'msl':'msl'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doublecheck the representation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if everything is sorted, renamed or removed correctly.\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also check if everything is sorted, renamed or removed correctly at the end of the\n",
    "# dataframe.\n",
    "df_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the ground truth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pandas dataframe in csv-format.\n",
    "df_data.to_csv(PATH_output_file+file_name_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of Program"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
