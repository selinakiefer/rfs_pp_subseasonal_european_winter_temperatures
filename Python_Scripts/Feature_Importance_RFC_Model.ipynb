{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Training Random Forest Classifier (RFC) Models\n",
    "\n",
    "Version 19 January 2024, Selina Kiefer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input: csv-files\n",
    "continuous timeseries of input data (e.g. statistics of meteorological predictor fields), binary timeseries of cold wave days in csv-format\n",
    "### Output: pt-file and txt-file\n",
    "Random Forest Classifier models in pt-format, file with metadata of the models in txt-format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the paths' to the defined functions and configuration file and set its name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the defined functions.\n",
    "PATH_defined_functions = './Defined_Functions/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path and name of the configuration file.\n",
    "PATH_configurations = './Configurations/'\n",
    "ifile_configurations = 'Configurations_RFC_Model.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the necessary python packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary python packages.\n",
    "import yaml\n",
    "import calendar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from skranger.ensemble import RangerForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed functions.\n",
    "import sys\n",
    "sys.path.insert(1, PATH_defined_functions)\n",
    "from read_in_csv_data import *\n",
    "from truncate_data_by_date import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the configuration file and the data specified in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the configuration file (nothing needs to be changed here).\n",
    "with open(PATH_configurations+ifile_configurations) as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If applicable, read in the reanalysis data and remove any unnamed columns as well as the index column.\n",
    "if config['use_reanalysis_data']: \n",
    "    df_input_data_era5 = read_in_csv_data(config['PATH_input_data_era5'], config['ifile_input_data_era5'])\n",
    "    df_input_data_era5 = df_input_data_era5.loc[:, ~df_input_data_era5.columns.str.contains('^Unnamed')]\n",
    "    df_input_data_era5 = df_input_data_era5.drop(['index'], axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If applicable, set the name of the columns containing the time and the variables of the reanalysis data.\n",
    "if config['use_reanalysis_data']: \n",
    "    time_column_name_input_data_era5 = df_input_data_era5.columns[0]\n",
    "    var_column_name_input_data_era5 = df_input_data_era5.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that everything is selected correctly for the reanalysis data if used.\n",
    "if config['use_reanalysis_data']: \n",
    "    print('Predictors used for training the ML model: ')\n",
    "    print(var_column_name_input_data_era5)\n",
    "    print('Name of the column containing the time: ')\n",
    "    print(time_column_name_input_data_era5)\n",
    "    print('Dataframe containing the predictors: ')\n",
    "    df_input_data_era5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the input data and remove any unnamed columns as well as the index column.\n",
    "df_input_data_s2s = read_in_csv_data(config['PATH_input_data_s2s'], config['ifile_input_data_s2s'])\n",
    "df_input_data_s2s = df_input_data_s2s.loc[:, ~df_input_data_s2s.columns.str.contains('^Unnamed')]\n",
    "df_input_data_s2s = df_input_data_s2s.drop(['index'], axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the columns containing the time and the variables of the input data.\n",
    "time_column_name_input_data_s2s = df_input_data_s2s.columns[0]\n",
    "var_column_name_input_data_s2s = df_input_data_s2s.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that everything is selected correctly.\n",
    "print('Predictors used for training the ML model: ')\n",
    "print(var_column_name_input_data_s2s)\n",
    "print('Name of the column containing the time: ')\n",
    "print(time_column_name_input_data_s2s)\n",
    "print('Dataframe containing the predictors: ')\n",
    "df_input_data_s2s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the ground truth and remove any unnamed columns as well as the index column.\n",
    "df_ground_truth = read_in_csv_data(config['PATH_ground_truth'], config['ifile_ground_truth'])\n",
    "df_ground_truth = df_ground_truth.loc[:, ~df_ground_truth.columns.str.contains('^Unnamed')]\n",
    "df_ground_truth = df_ground_truth.drop(['index'], axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the columns containing the time and the variables of the ground truth.\n",
    "time_column_name_ground_truth = df_ground_truth.columns[0]\n",
    "var_column_name_ground_truth = df_ground_truth.columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that everything is selected correctly.\n",
    "print('Predictand used for training the ML model: ')\n",
    "print(var_column_name_ground_truth)\n",
    "print('Name of the column containing the time: ')\n",
    "print(time_column_name_ground_truth)\n",
    "print('Dataframe containing the predictand: ')\n",
    "df_ground_truth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If reanalysis data is used, select only the dates which are present in both, the ERA5 input and the S2S reforecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the evaluation period from the input_data.\n",
    "if config['use_reanalysis_data']: \n",
    "    start_evaluation_period = datetime(config['start_year_of_first_winter'], config['start_month_winter'], config['start_day_winter'])\n",
    "    end_evaluation_period = datetime(config['start_year_of_last_winter']+1, config['end_month_winter'], config['end_day_winter'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column containing the time to datetime objects for both, the reanalysis data and the reforecast data.\n",
    "df_input_data_s2s[time_column_name_input_data_s2s] = pd.to_datetime(df_input_data_s2s[time_column_name_input_data_s2s])\n",
    "if config['use_reanalysis_data']: \n",
    "    df_input_data_era5[time_column_name_input_data_era5] = pd.to_datetime(df_input_data_era5[time_column_name_input_data_era5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the dates which are present in the S2S reforecasts ensemble and the reanalysis data. Consider thereby the \n",
    "# lead time since for the S2S reforecasts only the valid date is given and not the initial date.\n",
    "if config['use_reanalysis_data']: \n",
    "    dates_era5 = []\n",
    "\n",
    "    for i in range(len(df_input_data_era5[time_column_name_input_data_era5])):\n",
    "        dates_era5.append(df_input_data_era5[time_column_name_input_data_era5].iloc[i])\n",
    "\n",
    "    joint_dates = []\n",
    "    l = 0\n",
    "\n",
    "    for i in range(len(df_input_data_era5[time_column_name_input_data_era5])):\n",
    "        date_with_lead_time_considered = df_input_data_s2s[time_column_name_input_data_s2s].iloc[l]-timedelta(days=config['lead_time'])\n",
    "    \n",
    "        if df_input_data_era5[time_column_name_input_data_era5].iloc[i] == date_with_lead_time_considered:\n",
    "            joint_dates.append(date_with_lead_time_considered)\n",
    "            l = l+1\n",
    "            if l>len(df_input_data_s2s[time_column_name_input_data_s2s])-1:\n",
    "                l = 0\n",
    "            \n",
    "        elif date_with_lead_time_considered in dates_era5:\n",
    "    \n",
    "            joint_dates.append(np.nan)\n",
    "        \n",
    "        else:\n",
    "            joint_dates.append(np.nan)\n",
    "        \n",
    "            l = l+1\n",
    "            if l>len(df_input_data_s2s[time_column_name_input_data_s2s])-1:\n",
    "                l = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append these dates to the dataframe containing the reanalysis data.\n",
    "if config['use_reanalysis_data']: \n",
    "    df_input_data_era5['joint_dates'] = joint_dates\n",
    "    df_input_data_era5 = df_input_data_era5.dropna()\n",
    "    df_input_data_era5 = df_input_data_era5.drop(['joint_dates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine S2S and ERA5 predictors and set a new time column name.\n",
    "if config['use_reanalysis_data']: \n",
    "    df_input_data_era5 = df_input_data_era5.drop(time_column_name_input_data_era5, axis=1)\n",
    "    \n",
    "    columns_era5 = df_input_data_era5.columns\n",
    "    df_input_data = df_input_data_s2s\n",
    "    \n",
    "    for k in columns_era5:\n",
    "        df_input_data[k] = np.array(df_input_data_era5[k])\n",
    "        \n",
    "    time_column_name_input_data = time_column_name_input_data_s2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select only the dates which are present in both, the ground truth data and the S2S reforecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the evaluation period from the ground truth.\n",
    "start_evaluation_period = datetime(config['start_year_of_first_winter'], config['start_month_winter'], config['start_day_winter'])\n",
    "end_evaluation_period = datetime(config['start_year_of_last_winter']+1, config['end_month_winter'], config['end_day_winter'])\n",
    "\n",
    "df_ground_truth = truncate_data_by_date(df_ground_truth, time_column_name_ground_truth, start_evaluation_period.strftime('%Y_%m_%d'), end_evaluation_period.strftime('%Y_%m_%d')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the dates which are present in the input data and the ground truth data.\n",
    "joint_dates = []\n",
    "l = 0\n",
    "\n",
    "for i in range(len(df_ground_truth[time_column_name_ground_truth])):\n",
    "    if df_ground_truth[time_column_name_ground_truth].iloc[i].strftime('%Y-%m-%d') == df_input_data[time_column_name_input_data].iloc[l].strftime('%Y-%m-%d'):\n",
    "        joint_dates.append(df_ground_truth[time_column_name_ground_truth].iloc[i])\n",
    "        l = l+1\n",
    "        if l>len(df_input_data[time_column_name_input_data])-1:\n",
    "            l = 0\n",
    "    else:\n",
    "        joint_dates.append(np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append these dates to the dataframe containing the ground truth data.\n",
    "df_ground_truth['joint_dates'] = joint_dates\n",
    "df_ground_truth = df_ground_truth.dropna()\n",
    "df_ground_truth = df_ground_truth.drop(['joint_dates'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the winters to be evaluated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list with all the start years of the winters in the evaluation period is created. A\n",
    "# leave-one-out cross-validation will be used later to increase the amount of training data. \n",
    "start_years_of_winter = np.arange(config['start_year_of_first_winter'], config['start_year_of_last_winter']+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the input data for the leave-one(-winter)-out cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When performing a leave-one-out cross-validation appraoch, the training data needs to be\n",
    "# different for every left-out winter of the evaluation period. For an easy removal of the\n",
    "# winter to be left out, the time column of the input and the ground truth data is converted\n",
    "# to a datetime-object and then set as the index. \n",
    "df_input_data[time_column_name_input_data] = pd.to_datetime(df_input_data[time_column_name_input_data])\n",
    "df_input_data = df_input_data.set_index(time_column_name_input_data)\n",
    "\n",
    "df_ground_truth[time_column_name_ground_truth] = pd.to_datetime(df_ground_truth[time_column_name_ground_truth])\n",
    "df_ground_truth = df_ground_truth.set_index(time_column_name_ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training of the RFC-models with a leave-one(-winter)-out cross validation\n",
    "For every of these winters, a separate RFC model is trained and then saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, the actual training takes place. To perform a leave-one-out cross-validation, the \n",
    "# respective winter has to be cut out of the training data timeseries (.loc[]). Then, the \n",
    "# variable columns of the splitted training data (the one without the respective winter) is\n",
    "# written into a pandas dataframe for both, the input data and the ground truth. Now, the \n",
    "# Random Forest Classifier (RangerForestClassifier) is trained (fit()) and saved\n",
    "# (torch.save) for further use. This is done for every winter in the evaluation period \n",
    "# separately.\n",
    "feature_importances = []\n",
    "\n",
    "for start_year in start_years_of_winter:        \n",
    "    month_before_start_winter = datetime(start_year, config['start_month_winter']-1, config['start_day_winter'])\n",
    "    end_winter = datetime(start_year+1, config['end_month_winter'], config['end_day_winter'])\n",
    " \n",
    "    df_X_train = df_input_data.loc[(df_input_data.index < month_before_start_winter) | (df_input_data.index > end_winter)]    \n",
    "    df_y_train = df_ground_truth.loc[(df_ground_truth.index < month_before_start_winter) | (df_ground_truth.index > end_winter)]    \n",
    "    df_X_train = df_X_train.reset_index()\n",
    "    df_y_train = df_y_train.reset_index()\n",
    "    \n",
    "    df_y_train = df_y_train.drop([time_column_name_ground_truth], axis=1)\n",
    "    df_X_train = df_X_train.drop([time_column_name_input_data], axis=1)\n",
    "    \n",
    "    y_train = np.array(df_y_train)\n",
    "    X_train = np.array(df_X_train)   \n",
    "    \n",
    "    random_forest_classifier = RangerForestClassifier(n_estimators = 1000, min_node_size = 5, importance='impurity')\n",
    "    random_forest_classifier = random_forest_classifier.fit(X_train, np.squeeze(y_train))\n",
    "        #torch.save(random_forest_classifier, config['PATH_model']+'RFC_'+config['ground_truth']+'_'+config['location_ground_truth']+'_input_'+config['input_data']+'_'+config['location_input']+'_lead_'+str(config['lead_time'])+'d_without_'+str(start_year)+'_'+str(start_year+1)+'.pt')\n",
    "    print('Model without Winter '+str(start_year)+'/'+str(start_year+1)+' Trained.')\n",
    "    feature_importances.append(random_forest_classifier.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean feature importance.\n",
    "mean_feature_importance = np.mean(feature_importances, axis=0)\n",
    "np.shape(mean_feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 10 largest feature importances.\n",
    "indices_of_largest_feature_importances = np.argsort(mean_feature_importance, axis=0)[-10:]\n",
    "indices_of_largest_feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the 10 lagest feature importances.\n",
    "list_values_most_important_features = mean_feature_importance[indices_of_largest_feature_importances]\n",
    "list_names_most_important_features = df_X_train.columns[indices_of_largest_feature_importances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use different hatching for the predictors from reanalysis and reforecast data as well as a separate color when\n",
    "# t2m is the predictor.\n",
    "list_hatching = []\n",
    "list_colors = []\n",
    "\n",
    "for f in range(len(list_names_most_important_features)):\n",
    "    if 't2m' in list_names_most_important_features[f]:\n",
    "        list_hatching.append('/')\n",
    "        list_colors.append('thistle')\n",
    "\n",
    "    elif list_names_most_important_features[f].count('_') > 1:\n",
    "        list_hatching.append('/')\n",
    "        list_colors.append('cadetblue')\n",
    "    \n",
    "    else:\n",
    "        list_hatching.append('.')\n",
    "        list_colors.append('cadetblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the most important feature importances in a bar plot.\n",
    "plt.bar(list_names_most_important_features, list_values_most_important_features, color=list_colors, hatch=list_hatching, edgecolor='k')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.gca().invert_xaxis()\n",
    "plt.ylabel('Impurity Feature Importance Value')\n",
    "plt.title('RFC_stat_all_s2s_ens_era5, '+str(config['lead_time'])+' d lead')\n",
    "plt.savefig('/home/my6406/Desktop/RFs_with_ERA5_and_S2S_Reforecasts_as_Input/Data_and_Plots/Plots/RFC_stat_all_s2s_ens_era5_'+str(config['lead_time'])+'d.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of Program"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
