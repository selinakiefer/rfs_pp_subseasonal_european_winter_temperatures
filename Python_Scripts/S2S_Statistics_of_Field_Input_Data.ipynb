{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating \"Statistics of field\" from the Meteorological Predictor Fields as Input for RF-based ML-Models\n",
    "Version 19 January 2024, Selina Kiefer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input: csv-files\n",
    "continuous timeseries of S2S reforecasts' meteorological predictors as 2d-fields in csv-format\n",
    "### Output: csv-file\n",
    "continuous timeseries of the minimum, mean, maximum and variance of the meteorological predictors, of separate ensemble members and the overall ensemble information of these, per date in csv-format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the paths' and files' names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the needed path and file names.\n",
    "PATH_defined_functions = './Defined_Functions/'\n",
    "\n",
    "PATH_input_data = './Data_in_csv_Format/'\n",
    "ifiles_input_data = ['s2s_reforecasts_u10_60W_60E_20N_80N_2000_2020_lead_time_14d.csv',\n",
    "                    's2s_reforecasts_z100_60W_60E_20N_80N_2000_2020_lead_time_14d.csv',\n",
    "                    's2s_reforecasts_z300_60W_60E_20N_80N_2000_2020_lead_time_14d.csv',\n",
    "                    's2s_reforecasts_z500_60W_60E_20N_80N_2000_2020_lead_time_14d.csv',\n",
    "                    's2s_reforecasts_z850_60W_60E_20N_80N_2000_2020_lead_time_14d.csv',\n",
    "                    's2s_reforecasts_t850_60W_60E_20N_80N_2000_2020_lead_time_14d.csv',\n",
    "                    's2s_reforecasts_H850_60W_60E_20N_80N_2000_2020_lead_time_14d.csv',\n",
    "                    's2s_reforecasts_u300_60W_60E_20N_80N_2000_2020_lead_time_14d.csv',\n",
    "                    's2s_reforecasts_msl_60W_60E_20N_80N_2000_2020_lead_time_14d.csv']\n",
    "\n",
    "# We add the 2-meter temperature (= target variable of the forecast) of the S2S reforecasts as a predictor.\n",
    "PATH_target_variable = './Data_in_csv_Format/'\n",
    "ifile_target_variable = 'S2S_Reforecast_Ensemble_Lead_Time_14d_2000_2020.csv'\n",
    "\n",
    "PATH_output_file = './Data_in_csv_Format/'\n",
    "file_name_output_file_separate_fields = 's2s_reforecasts_statistics_separate_fields_u10_z100_z300_z500_z850_t850_H850_u300_msl_t2m_60W_60E_20N_80N_2000_2020_lead_time_14d.csv'\n",
    "file_name_output_file_ensemble_information = 's2s_reforecasts_statistics_ensemble_info_u10_z100_z300_z500_z850_t850_H850_u300_msl_t2m_60W_60E_20N_80N_2000_2020_lead_time_14d.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the necessary packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary python packages.\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary functions.\n",
    "import sys\n",
    "sys.path.insert(1,PATH_defined_functions)\n",
    "from read_in_csv_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the input data (one file as an example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in one input data file and show its content.\n",
    "df_input_data = read_in_csv_data(PATH_input_data,ifiles_input_data[0])\n",
    "df_input_data = df_input_data.drop(['index', 'Unnamed: 0'], axis=1)\n",
    "df_input_data = df_input_data.reset_index()\n",
    "df_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the target variable file and show its content. \n",
    "df_target_variable = read_in_csv_data(PATH_target_variable,ifile_target_variable)\n",
    "df_target_variable = df_target_variable.drop(['index', 'Unnamed: 0'], axis=1)\n",
    "df_target_variable = df_target_variable.reset_index()\n",
    "df_target_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  List the predictors to be combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the desired predictors and set how many of these should be taken from the first \n",
    "# dataframe. From all other dataframes, only 1 predictor is taken (if more are needed, list\n",
    "# these input files multiple times in \"ifiles_input_data\"). It is necessary to take the time as\n",
    "# a predictor since the data will be grouped by date later.\n",
    "target_variable = 't2m'\n",
    "desired_predictors = ['valid_time', 'number', 'latitude', 'longitude', 'u', 'gh', 'gh', 'gh', 'gh', 't', 'q', 'u', 'msl']\n",
    "desired_predictor_names = ['time', 'number', 'latitude', 'longitude', 'u10', 'z100', 'z300', 'z500', 'z850', 't850', 'H850', 'u300', 'msl']\n",
    "number_of_predictors_in_first_dataframe = 5\n",
    "time_column_name = 'time'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine all predictors into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new dataframe is created and the desired predictors from the first data file are written\n",
    "# into it.\n",
    "df_combined_input_data = pd.DataFrame()\n",
    "df_input_data = read_in_csv_data(PATH_input_data, ifiles_input_data[0])\n",
    "for i in range(number_of_predictors_in_first_dataframe):\n",
    "    df_combined_input_data[desired_predictor_names[i]] = df_input_data [desired_predictors[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From all other dataframes but the one with the target variable, the specified predictor is added to this new \n",
    "# dataframe.\n",
    "for k in range(len(ifiles_input_data)-1):\n",
    "    df_input_data = read_in_csv_data(PATH_input_data, ifiles_input_data[k+1])\n",
    "    df_combined_input_data[desired_predictor_names[i+k+1]] = df_input_data [desired_predictors[i+k+1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the statistics (minimum, mean, maximum and variance) of the predictor fields for each ensemble member separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, the time is set as the index and the data is grouped by date. For every desired statistic\n",
    "# of the field, the calculation is done directly after the grouping and written as separate\n",
    "# pandas series. Here, the minimum, mean, maximum and variance are calculated.\n",
    "df_statistics = pd.DataFrame()\n",
    "\n",
    "for i in range(11):\n",
    "    # Take every ensemble member separately.\n",
    "    df_combined_input_data_one_member = df_combined_input_data.where(df_input_data['number']==i)\n",
    "    df_combined_input_data_one_member = df_combined_input_data_one_member.dropna()\n",
    "    \n",
    "    # Calculate the statistics.\n",
    "    df_combined_input_data_one_member[time_column_name] = pd.to_datetime(df_combined_input_data_one_member[time_column_name])\n",
    "    df_combined_input_data_one_member = df_combined_input_data_one_member.set_index(time_column_name)\n",
    "    ds_input_data_one_member_grouped_min = df_combined_input_data_one_member.groupby([df_combined_input_data_one_member.index.year, df_combined_input_data_one_member.index.month, df_combined_input_data_one_member.index.day], as_index=False).min()\n",
    "    ds_input_data_one_member_grouped_mean = df_combined_input_data_one_member.groupby([df_combined_input_data_one_member.index.year, df_combined_input_data_one_member.index.month, df_combined_input_data_one_member.index.day], as_index=False).mean()\n",
    "    ds_input_data_one_member_grouped_max = df_combined_input_data_one_member.groupby([df_combined_input_data_one_member.index.year, df_combined_input_data_one_member.index.month, df_combined_input_data_one_member.index.day], as_index=False).max()\n",
    "    ds_input_data_one_member_grouped_var = df_combined_input_data_one_member.groupby([df_combined_input_data_one_member.index.year, df_combined_input_data_one_member.index.month, df_combined_input_data_one_member.index.day], as_index=False).var()\n",
    "    \n",
    "    # Add the results to a new dataframe.\n",
    "    for l in range(len(desired_predictor_names)-1):\n",
    "        df_statistics['min_'+desired_predictor_names[l+1]+'_'+str(i)] = ds_input_data_one_member_grouped_min[desired_predictor_names[l+1]]\n",
    "        df_statistics['mean_'+desired_predictor_names[l+1]+'_'+str(i)] = ds_input_data_one_member_grouped_mean[desired_predictor_names[l+1]]\n",
    "        df_statistics['max_'+desired_predictor_names[l+1]+'_'+str(i)] = ds_input_data_one_member_grouped_max[desired_predictor_names[l+1]]   \n",
    "        df_statistics['var_'+desired_predictor_names[l+1]+'_'+str(i)] = ds_input_data_one_member_grouped_var[desired_predictor_names[l+1]]\n",
    "\n",
    "    # Remove unmeaningful columns again.    \n",
    "    df_statistics = df_statistics.drop(['min_number'+'_'+str(i), 'mean_number'+'_'+str(i),'max_number'+'_'+str(i),'var_number'+'_'+str(i)], axis=1)\n",
    "    df_statistics = df_statistics.drop(['min_latitude'+'_'+str(i), 'mean_latitude'+'_'+str(i),'max_latitude'+'_'+str(i),'var_latitude'+'_'+str(i)], axis=1)\n",
    "    df_statistics = df_statistics.drop(['min_longitude'+'_'+str(i), 'mean_longitude'+'_'+str(i),'max_longitude'+'_'+str(i),'var_longitude'+'_'+str(i)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the month to the predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the time and the month from the input data.\n",
    "df_target_variable['valid_time'] = pd.to_datetime(df_target_variable['valid_time'])\n",
    "time = df_target_variable['valid_time']\n",
    "df_target_variable = df_target_variable.set_index('valid_time')\n",
    "month = df_target_variable.index.month\n",
    "df_target_variable = df_target_variable.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the month as predictor.\n",
    "df_statistics['month'] = month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add every ensemble member of the target variable to the predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this to the dataframe containing the statistics of the fields.\n",
    "for l in range(11):\n",
    "    df_statistics[target_variable+'_'+str(l)] =  df_target_variable[str(l)+'.0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the time information again to the reshaped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the next step, the time is added to the dataframe containing the statistics as predictors \n",
    "# (nothing needs to be changed here).\n",
    "df_statistics.insert(0, 'time', time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doublecheck the representation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if everything is reshaped and sorted correctly.\n",
    "df_statistics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also check if everything is sorted, renamed or removed correctly at the end of the\n",
    "# dataframe.\n",
    "df_statistics.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the statistics of every separate ensemble member in csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pandas dataframe in csv-format.\n",
    "df_statistics.to_csv(PATH_output_file+file_name_output_file_separate_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the ensemble information of the statistics of the predictor fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condense the information of the ensemble by calculating the minimum and variance of the minima of all ensemble\n",
    "# members, the mean and variance of the means and the variances of all ensemble members and the maximum and the\n",
    "# variance of the maxima of all ensemble members.\n",
    "df_statistics_ensemble = pd.DataFrame()\n",
    "\n",
    "for m in range(len(desired_predictor_names)-4):\n",
    "    selection = df_statistics.filter(regex=(''.join(['min_',desired_predictor_names[m+4],'*'])))\n",
    "    df_statistics_ensemble['min_min_'+desired_predictor_names[m+4]] = selection.min(axis=1)\n",
    "    df_statistics_ensemble['var_min_'+desired_predictor_names[m+4]] = selection.var(axis=1)\n",
    "    \n",
    "    selection = df_statistics.filter(regex=(''.join(['mean_',desired_predictor_names[m+4],'*'])))\n",
    "    df_statistics_ensemble['mean_mean_'+desired_predictor_names[m+4]] = selection.mean(axis=1)\n",
    "    df_statistics_ensemble['var_mean_'+desired_predictor_names[m+4]] = selection.var(axis=1)\n",
    "    \n",
    "    selection = df_statistics.filter(regex=(''.join(['max_',desired_predictor_names[m+4],'*'])))\n",
    "    df_statistics_ensemble['max_max_'+desired_predictor_names[m+4]] = selection.max(axis=1)\n",
    "    df_statistics_ensemble['var_max_'+desired_predictor_names[m+4]] = selection.var(axis=1)\n",
    "    \n",
    "    selection = df_statistics.filter(regex=(''.join(['var_',desired_predictor_names[m+4],'*'])))\n",
    "    df_statistics_ensemble['mean_var_'+desired_predictor_names[m+4]] = selection.mean(axis=1)\n",
    "    df_statistics_ensemble['var_var_'+desired_predictor_names[m+4]] = selection.var(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the month as predictor.\n",
    "df_statistics_ensemble['month'] = month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the statistics of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the statistics of the target variable to the dataframe containing the statistics of the fields.\n",
    "df_target_variable = df_target_variable.drop(['index', 'valid_time'], axis=1) \n",
    "\n",
    "df_statistics_ensemble['min_'+target_variable] = df_target_variable.min(axis=1)\n",
    "df_statistics_ensemble['mean_'+target_variable] = df_target_variable.mean(axis=1)\n",
    "df_statistics_ensemble['max_'+target_variable] = df_target_variable.max(axis=1)\n",
    "df_statistics_ensemble['var_'+target_variable] = df_target_variable.var(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the time information again to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the next step, the time is added to the dataframe containing the statistics as predictors.\n",
    "df_statistics_ensemble.insert(0, 'time', time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doublecheck the representation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if everything is reshaped and sorted correctly.\n",
    "df_statistics_ensemble.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also check if everything is sorted, renamed or removed correctly at the end of the\n",
    "# dataframe.\n",
    "df_statistics_ensemble.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the ensemble information of the statistics of every separate ensemble member in csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pandas dataframe in csv-format.\n",
    "df_statistics_ensemble.to_csv(PATH_output_file+file_name_output_file_ensemble_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of Program"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
