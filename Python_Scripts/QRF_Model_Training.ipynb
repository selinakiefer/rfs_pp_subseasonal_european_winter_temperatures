{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Training Postprocessing Quantile Random Forest (QRF) Models\n",
    "\n",
    "Version 19 January 2024, Selina Kiefer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input: csv-files\n",
    "continuous timeseries of input data (e.g. statistics of meteorological predictor fields), continuous timeseries of ground truth temperature in csv-format\n",
    "### Output: pt-file and txt-file\n",
    "Quantile Random Forests models in pt-format, file with metadata of the models in txt-format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the paths' to the defined functions and configuration file and set its name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the defined functions.\n",
    "PATH_defined_functions = './Defined_Functions/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path and name of the configuration file.\n",
    "PATH_configurations = './Configurations/'\n",
    "ifile_configurations = 'Configurations_QRF_Model.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the necessary python packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary python packages.\n",
    "import yaml\n",
    "import calendar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from skranger.ensemble import RangerForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed functions.\n",
    "import sys\n",
    "sys.path.insert(1, PATH_defined_functions)\n",
    "from read_in_csv_data import *\n",
    "from truncate_data_by_date import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the configuration file and the data specified in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the configuration file.\n",
    "with open(PATH_configurations+ifile_configurations) as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If applicable, read in the reanalysis data and remove any unnamed columns as well as the index column.\n",
    "if config['use_reanalysis_data']: \n",
    "    df_input_data_era5 = read_in_csv_data(config['PATH_input_data_era5'], config['ifile_input_data_era5'])\n",
    "    df_input_data_era5 = df_input_data_era5.loc[:, ~df_input_data_era5.columns.str.contains('^Unnamed')]\n",
    "    df_input_data_era5 = df_input_data_era5.drop(['index'], axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If applicable, set the name of the columns containing the time and the variables of the reanalysis data.\n",
    "if config['use_reanalysis_data']: \n",
    "    time_column_name_input_data_era5 = df_input_data_era5.columns[0]\n",
    "    var_column_name_input_data_era5 = df_input_data_era5.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that everything is selected correctly for the reanalysis data if used.\n",
    "if config['use_reanalysis_data']: \n",
    "    print('Predictors used for training the ML model: ')\n",
    "    print(var_column_name_input_data_era5)\n",
    "    print('Name of the column containing the time: ')\n",
    "    print(time_column_name_input_data_era5)\n",
    "    print('Dataframe containing the predictors: ')\n",
    "    df_input_data_era5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the reforecast data and remove any unnamed columns as well as the index column.\n",
    "df_input_data_s2s = read_in_csv_data(config['PATH_input_data_s2s'], config['ifile_input_data_s2s'])\n",
    "df_input_data_s2s = df_input_data_s2s.loc[:, ~df_input_data_s2s.columns.str.contains('^Unnamed')]\n",
    "df_input_data_s2s = df_input_data_s2s.drop(['index'], axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the columns containing the time and the variables of the reforecast data.\n",
    "time_column_name_input_data_s2s = df_input_data_s2s.columns[0]\n",
    "var_column_name_input_data_s2s = df_input_data_s2s.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that everything is selected correctly.\n",
    "print('Predictors used for training the ML model: ')\n",
    "print(var_column_name_input_data_s2s)\n",
    "print('Name of the column containing the time: ')\n",
    "print(time_column_name_input_data_s2s)\n",
    "print('Dataframe containing the predictors: ')\n",
    "df_input_data_s2s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the ground truth and remove any unnamed columns as well as the index column.\n",
    "df_ground_truth = read_in_csv_data(config['PATH_ground_truth'], config['ifile_ground_truth'])\n",
    "df_ground_truth = df_ground_truth.loc[:, ~df_ground_truth.columns.str.contains('^Unnamed')]\n",
    "df_ground_truth = df_ground_truth.drop(['index', 'level_0'], axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the columns containing the time and the variables of the ground truth.\n",
    "time_column_name_ground_truth = df_ground_truth.columns[0]\n",
    "var_column_name_ground_truth = df_ground_truth.columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that everything is selected correctly.\n",
    "print('Predictand used for training the ML model: ')\n",
    "print(var_column_name_ground_truth)\n",
    "print('Name of the column containing the time: ')\n",
    "print(time_column_name_ground_truth)\n",
    "print('Dataframe containing the predictand: ')\n",
    "df_ground_truth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If reanalysis data is used, select only the dates which are present in both, the ERA5 input and the S2S reforecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the evaluation period from the input_data.\n",
    "if config['use_reanalysis_data']: \n",
    "    start_evaluation_period = datetime(config['start_year_of_first_winter'], config['start_month_winter'], config['start_day_winter'])\n",
    "    end_evaluation_period = datetime(config['start_year_of_last_winter']+1, config['end_month_winter'], config['end_day_winter'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column containing the time to datetime objects for both, the reanalysis data and the reforecast data.\n",
    "df_input_data_s2s[time_column_name_input_data_s2s] = pd.to_datetime(df_input_data_s2s[time_column_name_input_data_s2s])\n",
    "if config['use_reanalysis_data']: \n",
    "    df_input_data_era5[time_column_name_input_data_era5] = pd.to_datetime(df_input_data_era5[time_column_name_input_data_era5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the dates which are present in the S2S reforecasts ensemble and the reanalysis data. Consider thereby the \n",
    "# lead time since for the S2S reforecasts only the valid date is given and not the initial date.\n",
    "if config['use_reanalysis_data']: \n",
    "    dates_era5 = []\n",
    "\n",
    "    for i in range(len(df_input_data_era5[time_column_name_input_data_era5])):\n",
    "        dates_era5.append(df_input_data_era5[time_column_name_input_data_era5].iloc[i])\n",
    "\n",
    "    joint_dates = []\n",
    "    l = 0\n",
    "\n",
    "    for i in range(len(df_input_data_era5[time_column_name_input_data_era5])):\n",
    "        date_with_lead_time_considered = df_input_data_s2s[time_column_name_input_data_s2s].iloc[l]-timedelta(days=config['lead_time'])\n",
    "    \n",
    "        if df_input_data_era5[time_column_name_input_data_era5].iloc[i] == date_with_lead_time_considered:\n",
    "            joint_dates.append(date_with_lead_time_considered)\n",
    "            l = l+1\n",
    "            if l>len(df_input_data_s2s[time_column_name_input_data_s2s])-1:\n",
    "                l = 0\n",
    "            \n",
    "        elif date_with_lead_time_considered in dates_era5:\n",
    "    \n",
    "            joint_dates.append(np.nan)\n",
    "        \n",
    "        else:\n",
    "            joint_dates.append(np.nan)\n",
    "        \n",
    "            l = l+1\n",
    "            if l>len(df_input_data_s2s[time_column_name_input_data_s2s])-1:\n",
    "                l = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append these dates to the dataframe containing the reanalysis data.\n",
    "if config['use_reanalysis_data']: \n",
    "    df_input_data_era5['joint_dates'] = joint_dates\n",
    "    df_input_data_era5 = df_input_data_era5.dropna()\n",
    "    df_input_data_era5 = df_input_data_era5.drop(['joint_dates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine S2S and ERA5 predictors and set a new time column name.\n",
    "if config['use_reanalysis_data']: \n",
    "    df_input_data_era5 = df_input_data_era5.drop(time_column_name_input_data_era5, axis=1)\n",
    "    \n",
    "    columns_era5 = df_input_data_era5.columns\n",
    "    df_input_data = df_input_data_s2s\n",
    "    \n",
    "    for k in columns_era5:\n",
    "        df_input_data[k] = np.array(df_input_data_era5[k])\n",
    "        \n",
    "    time_column_name_input_data = time_column_name_input_data_s2s\n",
    "    df_input_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select only the dates which are present in both, the ground truth data and the S2S reforecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the evaluation period from the ground truth.\n",
    "start_evaluation_period = datetime(config['start_year_of_first_winter'], config['start_month_winter'], config['start_day_winter'])\n",
    "end_evaluation_period = datetime(config['start_year_of_last_winter']+1, config['end_month_winter'], config['end_day_winter'])\n",
    "\n",
    "df_ground_truth = truncate_data_by_date(df_ground_truth, time_column_name_ground_truth, start_evaluation_period.strftime('%Y_%m_%d'), end_evaluation_period.strftime('%Y_%m_%d')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the dates which are present in the input data and the ground truth data.\n",
    "joint_dates = []\n",
    "l = 0\n",
    "\n",
    "for i in range(len(df_ground_truth[time_column_name_ground_truth])):\n",
    "    if df_ground_truth[time_column_name_ground_truth].iloc[i].strftime('%Y-%m-%d') == df_input_data[time_column_name_input_data].iloc[l].strftime('%Y-%m-%d'):\n",
    "        joint_dates.append(df_ground_truth[time_column_name_ground_truth].iloc[i])\n",
    "        l = l+1\n",
    "        if l>len(df_input_data[time_column_name_input_data])-1:\n",
    "            l = 0\n",
    "    else:\n",
    "        joint_dates.append(np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append these dates to the dataframe containing the ground truth data.\n",
    "df_ground_truth['joint_dates'] = joint_dates\n",
    "df_ground_truth = df_ground_truth.dropna()\n",
    "df_ground_truth = df_ground_truth.drop(['joint_dates'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the winters to be evaluated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list with all the start years of the winters in the evaluation period is created.\n",
    "start_years_of_winter = np.arange(config['start_year_of_first_winter'], config['start_year_of_last_winter']+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the input data for a leave-one(-winter)-out cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When performing a leave-one-out cross-validation appraoch, the training data needs to be\n",
    "# different for every left-out winter of the validation period. For an easy removal of the\n",
    "# winter to be left out, the time column of the input and the ground truth data is converted\n",
    "# to a datetime-object and then set as the index. \n",
    "df_input_data[time_column_name_input_data] = pd.to_datetime(df_input_data[time_column_name_input_data])\n",
    "df_input_data = df_input_data.set_index(time_column_name_input_data)\n",
    "\n",
    "df_ground_truth[time_column_name_ground_truth] = pd.to_datetime(df_ground_truth[time_column_name_ground_truth])\n",
    "df_ground_truth = df_ground_truth.set_index(time_column_name_ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training of the QRF-models with a leave-one(-winter)-out cross validation\n",
    "For every of these winters, a separate QRF model is trained and then saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, the actual training takes place. To perform a leave-one-out cross-validation, the \n",
    "# respective winter has to be cut out of the training data timeseries (.loc[]). Then, the \n",
    "# variable columns of the splitted training data (the one without the respective winter) is\n",
    "# written into a pandas dataframe for both, the input data and the ground truth. Now, the \n",
    "# Quantile (quantile=true) Random Forest (RangerForestRegressor) is trained (fit()) and saved\n",
    "# (torch.save) for further use. This is done for every winter in the evaluation period \n",
    "# separately.\n",
    "for start_year in start_years_of_winter:        \n",
    "    month_before_start_winter = datetime(start_year, config['start_month_winter']-1, config['start_day_winter'])\n",
    "    end_winter = datetime(start_year+1, config['end_month_winter'], config['end_day_winter'])\n",
    " \n",
    "    df_X_train = df_input_data.loc[(df_input_data.index < month_before_start_winter) | (df_input_data.index > end_winter)]    \n",
    "    df_y_train = df_ground_truth.loc[(df_ground_truth.index < month_before_start_winter) | (df_ground_truth.index > end_winter)]    \n",
    "    df_X_train = df_X_train.reset_index()\n",
    "    df_y_train = df_y_train.reset_index()\n",
    "    \n",
    "    df_y_train = df_y_train.drop([time_column_name_ground_truth], axis=1)\n",
    "    df_X_train = df_X_train.drop([time_column_name_input_data], axis=1)\n",
    "\n",
    "    y_train = np.array(df_y_train)\n",
    "    X_train = np.array(df_X_train)   \n",
    " \n",
    "    if config['obtain_additional_details_of_trees']:\n",
    "        quantile_regresssion_forest = RangerForestRegressor(n_estimators=1000, min_node_size=5, quantiles=True, enable_tree_details=True)\n",
    "        quantile_regresssion_forest = quantile_regresssion_forest.fit(X_train, np.squeeze(y_train))\n",
    "        torch.save(quantile_regresssion_forest, config['PATH_model']+'QRF_'+config['ground_truth']+'_'+config['location_ground_truth']+'_input_'+config['input_data']+'_'+config['location_input']+'_lead_'+str(config['lead_time'])+'d_without_'+str(start_year)+'_'+str(start_year+1)+'_with_tree_details.pt')        \n",
    "        print('Model without Winter '+str(start_year)+'/'+str(start_year+1)+' Trained.')\n",
    "    else: \n",
    "        quantile_regresssion_forest = RangerForestRegressor(n_estimators=1000, min_node_size=5, quantiles=True)\n",
    "        quantile_regresssion_forest = quantile_regresssion_forest.fit(X_train, np.squeeze(y_train))\n",
    "        torch.save(quantile_regresssion_forest, config['PATH_model']+'QRF_'+config['ground_truth']+'_'+config['location_ground_truth']+'_input_'+config['input_data']+'_'+config['location_input']+'_lead_'+str(config['lead_time'])+'d_without_'+str(start_year)+'_'+str(start_year+1)+'.pt')\n",
    "        print('Model without Winter '+str(start_year)+'/'+str(start_year+1)+' Trained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of the metadata as specified in the configuration file for all trained QRF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to combine every relevant information about the QRF model and the training process,\n",
    "# everything which cannot be inferred from the code is written in a list. This information\n",
    "# has to be given manually in the configuration file. \n",
    "additional_info_on_variables=['dataset_input : '+config['dataset_input_data'],\n",
    "                             'dataset_ground_truth: '+config['dataset_ground_truth'],\n",
    "                              'type_input_data: '+config['type_input_data'],\n",
    "                              'type_ground_truth: '+config['type_ground_truth'],\n",
    "                              'unit_of_ground_truth_and_prediction : '+config['unit_of_ground_truth_and_prediction'],\n",
    "                             'training_period: '+config['training_period'], \n",
    "                            'start_month_winter: '+str(config['start_month_winter']),\n",
    "                              'start_day_winter: '+str(config['start_day_winter']),\n",
    "                            'end_month_winter: '+str(config['end_month_winter']),\n",
    "                              'end_day_winter: '+str(config['end_day_winter']),\n",
    "                              'lead_time_in_days: '+str(config['lead_time']),\n",
    "                             'training_type: '+config['training_type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the metadata and the model parameters for all trained QRF models in one combined txt-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the relevant information about the QRF model and its training is combined and saved to a\n",
    "# txt-file. This is done only once for the whole evaluation period since the model setup is the\n",
    "# same for every winter. \n",
    "metadata_model = additional_info_on_variables\n",
    "qrf_hyperparameters = quantile_regresssion_forest.get_params()\n",
    "winter_left_out = 'validation_period_winters_left_out_one_at_a_time_: '+str(config['start_year_of_first_winter'])+'_'+str(config['start_year_of_last_winter']+1)\n",
    "metadata_model.append(winter_left_out)\n",
    "metadata_model.append('QRF_hyperparameters: '+str(qrf_hyperparameters))\n",
    "if config['obtain_additional_details_of_trees']:\n",
    "    file = open(config['PATH_model']+'QRF_'+config['ground_truth']+'_'+config['location_ground_truth']+'_input_'+config['input_data']+'_'+config['location_input']+'_lead_'+str(config['lead_time'])+'d_validation_'+str(config['start_year_of_first_winter'])+'_'+str(config['start_year_of_last_winter']+1)+'_with_tree_details.txt', 'w') \n",
    "else: \n",
    "    file = open(config['PATH_model']+'QRF_'+config['ground_truth']+'_'+config['location_ground_truth']+'_input_'+config['input_data']+'_'+config['location_input']+'_lead_'+str(config['lead_time'])+'d_validation_'+str(config['start_year_of_first_winter'])+'_'+str(config['start_year_of_last_winter']+1)+'.txt', 'w') \n",
    "file.write('\\n'.join(metadata_model))\n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of Program"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
