{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating \"PCA of field\" from the Meteorological Predictor Fields as Input for RF-based ML-Models\n",
    "Version 19 January 2024, Selina Kiefer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input: csv-files\n",
    "continuous timeseries of S2S reforecasts' meteorological predictors as 2d-fields in csv-format\n",
    "### Output: csv-file\n",
    "continuous timeseries of the first 10 principle comonents of the meteorological predictors, of separate ensemble members and the mean and variance of these, per date in csv-format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the paths' and files' names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the needed path and file names.\n",
    "PATH_defined_functions = './Defined_Functions/'\n",
    "\n",
    "PATH_input_data = './Data_in_csv_Format/'\n",
    "ifiles_input_data = ['s2s_reforecasts_u10_60W_60E_20N_80N_2000_2020_lead_time_14d.csv',\n",
    "                    's2s_reforecasts_z100_60W_60E_20N_80N_2000_2020_lead_time_14d.csv',\n",
    "                    's2s_reforecasts_z300_60W_60E_20N_80N_2000_2020_lead_time_14d.csv',\n",
    "                    's2s_reforecasts_z500_60W_60E_20N_80N_2000_2020_lead_time_14d.csv',\n",
    "                    's2s_reforecasts_z850_60W_60E_20N_80N_2000_2020_lead_time_14d.csv',\n",
    "                    's2s_reforecasts_t850_60W_60E_20N_80N_2000_2020_lead_time_14d.csv',\n",
    "                    's2s_reforecasts_H850_60W_60E_20N_80N_2000_2020_lead_time_14d.csv',\n",
    "                    's2s_reforecasts_u300_60W_60E_20N_80N_2000_2020_lead_time_14d.csv',\n",
    "                    's2s_reforecasts_msl_60W_60E_20N_80N_2000_2020_lead_time_14d.csv']\n",
    "\n",
    "# We add the 2-meter temperature (= target variable of the forecast) of the S2S reforecasts as a predictor.\n",
    "PATH_target_variable = './Data_in_csv_Format/'\n",
    "ifile_target_variable = 'S2S_Reforecast_Ensemble_Lead_Time_14d_2000_2020.csv'\n",
    "\n",
    "PATH_output_file = './Data_in_csv_Format/'\n",
    "file_name_output_file_separate_fields = 's2s_reforecasts_pca_n10_separate_fields_u10_z100_z300_z500_z850_t850_H850_u300_msl_t2m_60W_60E_20N_80N_2000_2020_lead_time_14d.csv'\n",
    "file_name_output_file_ensemble_information = 's2s_reforecasts_pca_n10_ensemble_info_u10_z100_z300_z500_z850_t850_H850_u300_msl_t2m_60W_60E_20N_80N_2000_2020_lead_time_14d.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the necessary packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary python packages.\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary functions.\n",
    "import sys\n",
    "sys.path.insert(1,PATH_defined_functions)\n",
    "from read_in_csv_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the input data (one file as an example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in one input data file and show its content.\n",
    "df_input_data = read_in_csv_data(PATH_input_data,ifiles_input_data[0])\n",
    "df_input_data = df_input_data.drop(['index', 'Unnamed: 0'], axis=1)\n",
    "df_input_data = df_input_data.reset_index()\n",
    "df_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the target variable file and show its content. \n",
    "df_target_variable = read_in_csv_data(PATH_target_variable,ifile_target_variable)\n",
    "df_target_variable = df_target_variable.drop(['index', 'Unnamed: 0'], axis=1)\n",
    "df_target_variable = df_target_variable.reset_index()\n",
    "df_target_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  List the predictors to be combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the desired predictors and set how many of these should be taken from the first \n",
    "# dataframe. From all other dataframes, only 1 predictor is taken (if more are needed, list\n",
    "# these input files multiple times in \"ifiles_input_data\"). It is necessary to take the time as\n",
    "# a predictor since the data will be grouped by date later.\n",
    "target_variable = 't2m'\n",
    "desired_predictors = ['valid_time', 'number', 'latitude', 'longitude', 'u', 'gh', 'gh', 'gh', 'gh', 't', 'q', 'u', 'msl']\n",
    "desired_predictor_names = ['time', 'number', 'latitude', 'longitude', 'u10', 'z100', 'z300', 'z500', 'z850', 't850', 'H850', 'u300', 'msl']\n",
    "number_of_predictors_in_first_dataframe = 5\n",
    "time_column_name = 'valid_time'\n",
    "number_of_latitudes = 41\n",
    "number_of_longitudes = 81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decide how many principle components should be calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set how many components should be used by the Principal Components Analysis (PCA). \n",
    "number_of_principle_components = 10\n",
    "pca = PCA(n_components=number_of_principle_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the PCA of the predictor fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The PCA is performed for every day and every predictor field separately. \n",
    "# Therefore, one dataframe is read in and grouped by year, month and days (.groupby()). Then, \n",
    "# one day is selected (.iloc[]) and from the resulting dataframe only the predictor's column is\n",
    "# taken and converted into a numpy array. Then, this column is reshaped into the dimensions of\n",
    "# a field again (.reshape(latitude, longitude)). Then, the PCA is performed (pca.fit_transform).\n",
    "# From the PCA, the singular values (= PCA loadings) are taken and written in to a list. This\n",
    "# list is then appended to list containing all singular values of the predictor field for all\n",
    "# days and from this list, all the singular values are written into a pandas dataframe.\n",
    "field_one_variable_all_days = []\n",
    "df_input_data_pca = pd.DataFrame()\n",
    "\n",
    "for i in range(len(ifiles_input_data)):\n",
    "    # Consider every predictor separately.\n",
    "    df_input_data_one_variable = read_in_csv_data(PATH_input_data, ifiles_input_data[i])   \n",
    "    \n",
    "    # Also consider every ensemble member separately.\n",
    "    for m in range(11):\n",
    "\n",
    "        df_input_data_one_variable_one_member = df_input_data_one_variable.where(df_input_data['number']==m)\n",
    "        df_input_data_one_variable_one_member = df_input_data_one_variable_one_member.dropna()\n",
    "    \n",
    "        # Consider every date separately.\n",
    "        df_input_data_one_variable_one_member[time_column_name] = pd.to_datetime(df_input_data_one_variable_one_member[time_column_name])\n",
    "        df_input_data_one_variable_one_member = df_input_data_one_variable_one_member.set_index(time_column_name)\n",
    "        ds_input_data_one_variable_grouped = df_input_data_one_variable_one_member.groupby([df_input_data_one_variable_one_member.index.year, df_input_data_one_variable_one_member.index.month, df_input_data_one_variable_one_member.index.day], as_index=False)\n",
    "    \n",
    "        df_input_data_one_variable_grouped = pd.DataFrame(ds_input_data_one_variable_grouped)\n",
    "\n",
    "        for k in range(len(df_input_data_one_variable_grouped)):\n",
    "            df_input_data_one_variable_one_day = df_input_data_one_variable_grouped.iloc[k]\n",
    "            df_input_data_one_variable_one_day = df_input_data_one_variable_one_day[1]\n",
    "\n",
    "            field_one_variable_one_day = np.array(df_input_data_one_variable_one_day[desired_predictors[i+4]])\n",
    "            \n",
    "            # Reshape the data into a 2d representation, datewise.\n",
    "            field_one_variable_one_day = field_one_variable_one_day.reshape(number_of_latitudes,number_of_longitudes)\n",
    "            field_one_variable_all_days.append(field_one_variable_one_day)\n",
    "        \n",
    "        field_one_variable_all_days = np.array(field_one_variable_all_days)\n",
    "        \n",
    "        # Reshape the data again to perform the PCA for all days.\n",
    "        field_one_variable_all_days = field_one_variable_all_days.reshape(( -1, number_of_latitudes*number_of_longitudes))\n",
    "    \n",
    "        field_one_variable_all_days_fitted = pca.fit_transform(field_one_variable_all_days)\n",
    "              \n",
    "        field_one_variable_all_days_transformed = pca.transform(field_one_variable_all_days)\n",
    "   \n",
    "        field_one_variable_all_days = []\n",
    "    \n",
    "        # Add the first 10 principle components to a new dataframe.    \n",
    "        for l in range(number_of_principle_components):    \n",
    "            df_input_data_pca[desired_predictor_names[i+4]+'_'+str(m)+'_n'+str(l+1)] = field_one_variable_all_days_transformed[:,l]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the month to the predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the time and the month from the target variable data.\n",
    "df_target_variable['valid_time'] = pd.to_datetime(df_target_variable['valid_time'])\n",
    "time = df_target_variable['valid_time']\n",
    "df_target_variable = df_target_variable.set_index('valid_time')\n",
    "month = df_target_variable.index.month\n",
    "df_target_variable = df_target_variable.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the month as predictor.\n",
    "df_input_data_pca['month'] = month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add every ensemble member of the target variable to the predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this to the dataframe containing the statistics of the fields.\n",
    "for l in range(11):\n",
    "    df_input_data_pca[target_variable+'_'+str(l)] =  df_target_variable[str(l)+'.0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the time information again to the reshaped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the next step, the time is added to the dataframe containing the statistics as predictors \n",
    "# (nothing needs to be changed here).\n",
    "df_input_data_pca.insert(0, 'time', time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doublecheck the representation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if everything is reshaped and sorted correctly.\n",
    "df_input_data_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also check if everything is sorted, renamed or removed correctly at the end of the\n",
    "# dataframe.\n",
    "df_input_data_pca.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the first 10 principle components of every separate ensemble member in csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pandas dataframe in csv-format.\n",
    "df_input_data_pca.to_csv(PATH_output_file+file_name_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the ensemble mean and variance of the first 10 principle components of ensemble members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condense the information of the ensemble by taking the mean and variance of the first 10 principl components of\n",
    "# the ensemble members.\n",
    "df_input_data_pca_ensemble = pd.DataFrame()\n",
    "\n",
    "for m in range(len(desired_predictor_names)-4):\n",
    "    selection = df_input_data_pca.filter(regex=(''.join([desired_predictor_names[m+4],'_n1_', '.*'])))\n",
    "    df_input_data_pca_ensemble['mean_n1_'+desired_predictor_names[m+4]] = selection.min(axis=1)\n",
    "    df_input_data_pca_ensemble['var_n1_'+desired_predictor_names[m+4]] = selection.var(axis=1)\n",
    "    \n",
    "    selection = df_input_data_pca.filter(regex=(''.join([desired_predictor_names[m+4],'_n2_', '.*'])))\n",
    "    df_input_data_pca_ensemble['mean_n2_'+desired_predictor_names[m+4]] = selection.min(axis=1)\n",
    "    df_input_data_pca_ensemble['var_n2_'+desired_predictor_names[m+4]] = selection.var(axis=1)\n",
    "    \n",
    "    selection = df_input_data_pca.filter(regex=(''.join([desired_predictor_names[m+4],'_n3_', '.*'])))\n",
    "    df_input_data_pca_ensemble['mean_n3_'+desired_predictor_names[m+4]] = selection.min(axis=1)\n",
    "    df_input_data_pca_ensemble['var_n3_'+desired_predictor_names[m+4]] = selection.var(axis=1)\n",
    "    \n",
    "    selection = df_input_data_pca.filter(regex=(''.join([desired_predictor_names[m+4],'_n4_', '.*'])))\n",
    "    df_input_data_pca_ensemble['mean_n4_'+desired_predictor_names[m+4]] = selection.min(axis=1)\n",
    "    df_input_data_pca_ensemble['var_n4_'+desired_predictor_names[m+4]] = selection.var(axis=1)\n",
    "    \n",
    "    selection = df_input_data_pca.filter(regex=(''.join([desired_predictor_names[m+4],'_n5_', '.*'])))\n",
    "    df_input_data_pca_ensemble['mean_n5_'+desired_predictor_names[m+4]] = selection.min(axis=1)\n",
    "    df_input_data_pca_ensemble['var_n5_'+desired_predictor_names[m+4]] = selection.var(axis=1)\n",
    "    \n",
    "    selection = df_input_data_pca.filter(regex=(''.join([desired_predictor_names[m+4],'_n6_', '.*'])))\n",
    "    df_input_data_pca_ensemble['mean_n6_'+desired_predictor_names[m+4]] = selection.min(axis=1)\n",
    "    df_input_data_pca_ensemble['var_n6_'+desired_predictor_names[m+4]] = selection.var(axis=1)\n",
    "    \n",
    "    selection = df_input_data_pca.filter(regex=(''.join([desired_predictor_names[m+4],'_n7_', '.*'])))\n",
    "    df_input_data_pca_ensemble['mean_n7_'+desired_predictor_names[m+4]] = selection.min(axis=1)\n",
    "    df_input_data_pca_ensemble['var_n7_'+desired_predictor_names[m+4]] = selection.var(axis=1)\n",
    "    \n",
    "    selection = df_input_data_pca.filter(regex=(''.join([desired_predictor_names[m+4],'_n8_', '.*'])))\n",
    "    df_input_data_pca_ensemble['mean_n8_'+desired_predictor_names[m+4]] = selection.min(axis=1)\n",
    "    df_input_data_pca_ensemble['var_n8_'+desired_predictor_names[m+4]] = selection.var(axis=1)\n",
    "    \n",
    "    selection = df_input_data_pca.filter(regex=(''.join([desired_predictor_names[m+4],'_n9_', '.*'])))\n",
    "    df_input_data_pca_ensemble['mean_n9_'+desired_predictor_names[m+4]] = selection.min(axis=1)\n",
    "    df_input_data_pca_ensemble['var_n9_'+desired_predictor_names[m+4]] = selection.var(axis=1)\n",
    "    \n",
    "    selection = df_input_data_pca.filter(regex=(''.join([desired_predictor_names[m+4],'_n10_', '.*'])))\n",
    "    df_input_data_pca_ensemble['mean_n10_'+desired_predictor_names[m+4]] = selection.min(axis=1)\n",
    "    df_input_data_pca_ensemble['var_n10_'+desired_predictor_names[m+4]] = selection.var(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the month to the condensed predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the month as predictor.\n",
    "df_input_data_pca_ensemble['month'] = month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the statistics of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the statistics of the target variable to the dataframe containing the statistics of the fields.\n",
    "df_target_variable = df_target_variable.drop(['index', 'valid_time'], axis=1) \n",
    "\n",
    "df_input_data_pca_ensemble['min_'+target_variable] = df_target_variable.min(axis=1)\n",
    "df_input_data_pca_ensemble['mean_'+target_variable] = df_target_variable.mean(axis=1)\n",
    "df_input_data_pca_ensemble['max_'+target_variable] = df_target_variable.max(axis=1)\n",
    "df_input_data_pca_ensemble['var_'+target_variable] = df_target_variable.var(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the time information again to the predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the next step, the time is added to the dataframe containing the statistics as predictors.\n",
    "df_input_data_pca_ensemble.insert(0, 'time', time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doublecheck the representation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the beginning of the dataframe.\n",
    "df_input_data_pca_ensemble.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the end of the dataframe.\n",
    "df_input_data_pca_ensemble.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the ensemble mean and variance of the first 10 principle components of the ensemble member in csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pandas dataframe in csv-format.\n",
    "df_input_data_pca_ensemble.to_csv(PATH_output_file+file_name_output_file_ensemble_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of Program"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
