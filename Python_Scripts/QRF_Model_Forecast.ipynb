{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting with Quantile Random Forest (QRF) Models\n",
    "\n",
    "Version 19 January 2024, Selina Kiefer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input: csv-file, pt-file\n",
    "continuous timeseries of input data (e.g. statistics of meteorological predictor fields), Quantile Random Forests models in pt-format\n",
    "### Output: csv-file, png-file\n",
    "predictions of the Quantile Random Forest models as continuous timeseries of temperature in csv-format  and plotted for one winter exemplarily in png-format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the paths' to the defined functions and configuration file and set its name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the defined functions.\n",
    "PATH_defined_functions = './Defined_Functions/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path and name of the configuration file.\n",
    "PATH_configurations = './Configurations/'\n",
    "ifile_configurations = 'Configurations_QRF_Forecast.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the necessary python packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary python packages.\n",
    "import yaml\n",
    "import calendar\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from skranger.ensemble import RangerForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary python packages and functions.\n",
    "import sys\n",
    "sys.path.insert(1, PATH_defined_functions)\n",
    "from read_in_csv_data import *\n",
    "from truncate_data_by_date import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the configuration file and the data specified in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the configuration file (nothing needs to be changed here).\n",
    "with open(PATH_configurations+ifile_configurations) as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If applicable, read in the reanalysis data and remove any unnamed columns as well as the index column.\n",
    "if config['use_reanalysis_data']: \n",
    "    df_input_data_era5 = read_in_csv_data(config['PATH_input_data_era5'], config['ifile_input_data_era5'])\n",
    "    df_input_data_era5 = df_input_data_era5.loc[:, ~df_input_data_era5.columns.str.contains('^Unnamed')]\n",
    "    df_input_data_era5 = df_input_data_era5.drop(['index'], axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If applicable, set the name of the columns containing the time and the variables of the reanalysis data.\n",
    "if config['use_reanalysis_data']: \n",
    "    time_column_name_input_data_era5 = df_input_data_era5.columns[0]\n",
    "    var_column_name_input_data_era5 = df_input_data_era5.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that everything is selected correctly for the reanalysis data if used.\n",
    "if config['use_reanalysis_data']: \n",
    "    print('Predictors used for training the ML model: ')\n",
    "    print(var_column_name_input_data_era5)\n",
    "    print('Name of the column containing the time: ')\n",
    "    print(time_column_name_input_data_era5)\n",
    "    print('Dataframe containing the p_era5redictors: ')\n",
    "    df_input_data_era5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the input data and remove any unnamed columns as well as the index column.\n",
    "df_input_data_s2s = read_in_csv_data(config['PATH_input_data_s2s'], config['ifile_input_data_s2s'])\n",
    "df_input_data_s2s = df_input_data_s2s.loc[:, ~df_input_data_s2s.columns.str.contains('^Unnamed')]\n",
    "df_input_data_s2s = df_input_data_s2s.drop(['index'], axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the columns containing the time and the variables of the input data.\n",
    "time_column_name_input_data_s2s = df_input_data_s2s.columns[0]\n",
    "var_column_name_input_data_s2s = df_input_data_s2s.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that everything is selected correctly.\n",
    "print('Predictors used for training the ML model: ')\n",
    "print(var_column_name_input_data_s2s)\n",
    "print('Name of the column containing the time: ')\n",
    "print(time_column_name_input_data_s2s)\n",
    "print('Dataframe containing the p_s2sredictors: ')\n",
    "df_input_data_s2s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If reanalysis data is used, select only the dates which are present in both, the ERA5 input and the S2S reforecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the evaluation period from the input_data.\n",
    "if config['use_reanalysis_data']: \n",
    "    start_evaluation_period = datetime(config['start_year_of_first_winter'], config['start_month_winter'], config['start_day_winter'])\n",
    "    end_evaluation_period = datetime(config['start_year_of_last_winter']+1, config['end_month_winter'], config['end_day_winter'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column containing the time to datetime objects for both, the reanalysis data and the reforecast data.\n",
    "df_input_data_s2s[time_column_name_input_data_s2s] = pd.to_datetime(df_input_data_s2s[time_column_name_input_data_s2s])\n",
    "if config['use_reanalysis_data']: \n",
    "    df_input_data_era5[time_column_name_input_data_era5] = pd.to_datetime(df_input_data_era5[time_column_name_input_data_era5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the dates which are present in the S2S reforecasts ensemble and the reanalysis data. Consider thereby the \n",
    "# lead time since for the S2S reforecasts only the valid date is given and not the initial date.\n",
    "if config['use_reanalysis_data']: \n",
    "    dates_era5 = []\n",
    "\n",
    "    for i in range(len(df_input_data_era5[time_column_name_input_data_era5])):\n",
    "        dates_era5.append(df_input_data_era5[time_column_name_input_data_era5].iloc[i])\n",
    "\n",
    "    joint_dates = []\n",
    "    l = 0\n",
    "\n",
    "    for i in range(len(df_input_data_era5[time_column_name_input_data_era5])):\n",
    "        date_with_lead_time_considered = df_input_data_s2s[time_column_name_input_data_s2s].iloc[l]-timedelta(days=config['lead_time'])\n",
    "    \n",
    "        if df_input_data_era5[time_column_name_input_data_era5].iloc[i] == date_with_lead_time_considered:\n",
    "            joint_dates.append(date_with_lead_time_considered)\n",
    "            l = l+1\n",
    "            if l>len(df_input_data_s2s[time_column_name_input_data_s2s])-1:\n",
    "                l = 0\n",
    "            \n",
    "        elif date_with_lead_time_considered in dates_era5:\n",
    "    \n",
    "            joint_dates.append(np.nan)\n",
    "        \n",
    "        else:\n",
    "            joint_dates.append(np.nan)\n",
    "        \n",
    "            l = l+1\n",
    "            if l>len(df_input_data_s2s[time_column_name_input_data_s2s])-1:\n",
    "                l = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append these dates to the dataframe containing the reanalysis data.\n",
    "if config['use_reanalysis_data']: \n",
    "    df_input_data_era5['joint_dates'] = joint_dates\n",
    "    df_input_data_era5 = df_input_data_era5.dropna()\n",
    "    df_input_data_era5 = df_input_data_era5.drop(['joint_dates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine S2S and ERA5 predictors and set a new time column name.\n",
    "if config['use_reanalysis_data']: \n",
    "    df_input_data_era5 = df_input_data_era5.drop(time_column_name_input_data_era5, axis=1)\n",
    "    \n",
    "    columns_era5 = df_input_data_era5.columns\n",
    "    df_input_data = df_input_data_s2s\n",
    "\n",
    "    for k in columns_era5:\n",
    "        df_input_data[k] = np.array(df_input_data_era5[k])\n",
    "        \n",
    "    time_column_name_input_data = time_column_name_input_data_s2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the input data for forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list with all the start years of the winters to be predicted is created. \n",
    "start_years_of_winter = np.arange(config['start_year_of_first_winter'], config['start_year_of_last_winter']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to extract the different winters to be predicted, the index of the dataframe\n",
    "# containing the input data is set to the time. The time column is converted beforehand into a\n",
    "# datetime-object.\n",
    "df_input_data[time_column_name_input_data] = pd.to_datetime(df_input_data[time_column_name_input_data])\n",
    "df_input_data = df_input_data.set_index(time_column_name_input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the quantiles used for predicting by the QRF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For predicting with the QRF-model, a list with the desired quantiles is created. \n",
    "if config['distributed_evenly']:\n",
    "    list_quantiles_qrf = list(np.round(np.linspace(0, 1,config['number_of_quantiles']), decimals=2))\n",
    "else:\n",
    "    list_quantiles_qrf = config['list_quantiles_qrf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forecasting with the QRF-models\n",
    "For every winter to be predicted, the respective QRF model trained with the leave-one(-winter)-out cross-validation is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, the forecasting with the QRFs takes place. At first, the model used for forecasting the \n",
    "# respective winter is loaded. Then, the start- and end-date of this winter is determined and\n",
    "# the number of days which have to be taken from the month before the winter. This is done to\n",
    "# take the lead time into account and start the prediction with the first day of the winter.\n",
    "# In a next steps, the days of winter are computed in order to create a list of forecast dates.\n",
    "# These are needed to assign the predictions of the QRF to a date later. Then, the respective\n",
    "# winter is extracted from the input data (.loc[]) and the time column removed. Now, the loaded\n",
    "# QRF model is used to predict the desired quantiles. These predictions are saved to a list.\n",
    "# This is done for every winter in the evaluation period separately.\n",
    "predictions = []\n",
    "forecast_dates = []\n",
    "\n",
    "for i in range(len(start_years_of_winter)):\n",
    "    \n",
    "    quantile_regresssion_forest = torch.load(config['PATH_model']+config['list_file_name_model'][i])\n",
    "    \n",
    "    start_winter = datetime(start_years_of_winter[i], config['start_month_winter'], config['start_day_winter'])\n",
    "    month_before_start_winter = datetime(start_years_of_winter[i], config['start_month_winter']-1, config['start_day_winter'])\n",
    "    end_winter = datetime(start_years_of_winter[i]+1, config['end_month_winter'], config['end_day_winter'])\n",
    "    \n",
    "    df_X_val = df_input_data.loc[(df_input_data.index > month_before_start_winter) & (df_input_data.index < end_winter)]    \n",
    "    df_X_val = df_X_val.reset_index()\n",
    "    \n",
    "    forecast_dates_winter = df_X_val[time_column_name_input_data]\n",
    "\n",
    "    X_val = df_X_val.drop([time_column_name_input_data], axis=1)\n",
    "    predictions_qrf = quantile_regresssion_forest.predict_quantiles(X_val, quantiles=list_quantiles_qrf)\n",
    "    print('Predictions for Winter '+str(start_years_of_winter[i])+'/'+str(start_years_of_winter[i]+1)+' Made.')\n",
    "\n",
    "    predictions.append(predictions_qrf)\n",
    "    forecast_dates.extend(forecast_dates_winter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bringing the forecasts into a nice representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a nice representation of the predictions in a pandas dataframe, the list containing the\n",
    "# predictions is transposed, so that each predicted winter can be extracted easily. For every\n",
    "# day of the winter, the respective forecast is stored first as a numpy array and then \n",
    "# appended to a new dataframe which contains every prediction in a nicely sorted way.\n",
    "df_predictions = pd.DataFrame()\n",
    "\n",
    "for k in range(len(start_years_of_winter)):\n",
    "    predictions_single_winter = np.array(predictions[k])\n",
    "    \n",
    "    for l in range(52):\n",
    "            predictions_daily = np.array(predictions_single_winter[l])\n",
    "            df_predictions = df_predictions.append(pd.Series(predictions_daily), ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To this dataframe, the forecast dates are added and moved to the beginning of the dataframe\n",
    "# for a good overview.\n",
    "df_predictions['time'] = forecast_dates\n",
    "time_column = df_predictions.pop('time')\n",
    "df_predictions.insert(0, 'time', time_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the predictions in csv-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, the dataframe is saved in csv-format.\n",
    "df_predictions.to_csv(config['PATH_predictions']+'QRF_predictions_'+config['ground_truth']+'_'+config['location_ground_truth']+'_input_'+config['input_data']+'_'+config['location_input']+'_lead_'+str(config['lead_time'])+'d_winter_'+str(config['start_year_of_first_winter'])+'_'+str(config['start_year_of_last_winter']+1)+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the predictions of one winter for a plausibility check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, the first predicted winter is plotted for a plausibility check.\n",
    "df_predictions.iloc[0:52].plot(x='time', legend=False, marker='o', linestyle='')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel(config['ground_truth']+' in '+config['unit_of_ground_truth_and_prediction'])\n",
    "plt.title('QRF Predictions for the Winter '+str(config['start_year_of_first_winter'])+'/'+str(config['start_year_of_first_winter']+1))\n",
    "plt.savefig(config['PATH_predictions']+'QRF_predictions_'+config['ground_truth']+'_'+config['location_ground_truth']+'_input_'+config['input_data']+'_'+config['location_input']+'_lead_'+str(config['lead_time'])+'d_winter_'+str(config['start_year_of_first_winter'])+'_'+str(config['start_year_of_first_winter']+1)+'.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of Program"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
